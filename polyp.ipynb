{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-12-16T08:00:27.498667Z",
     "iopub.status.busy": "2025-12-16T08:00:27.498435Z",
     "iopub.status.idle": "2025-12-16T08:00:44.064853Z",
     "shell.execute_reply": "2025-12-16T08:00:44.064135Z",
     "shell.execute_reply.started": "2025-12-16T08:00:27.498646Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import SegformerForSemanticSegmentation, SegformerImageProcessor\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:02.301862Z",
     "iopub.status.busy": "2025-12-16T08:12:02.301469Z",
     "iopub.status.idle": "2025-12-16T08:12:02.328034Z",
     "shell.execute_reply": "2025-12-16T08:12:02.327275Z",
     "shell.execute_reply.started": "2025-12-16T08:12:02.301830Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Number of class in the data set (3: neoplastic, non neoplastic, background)\n",
    "NUM_CLASSES = 3\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 30\n",
    "LR = 1e-4\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Model path\n",
    "checkpoint_path = '/kaggle/working/segformer.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose,\n",
    "    RandomRotate90,\n",
    "    Transpose,\n",
    "    ElasticTransform,\n",
    "    GridDistortion,\n",
    "    OpticalDistortion,\n",
    "    RandomBrightnessContrast,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,\n",
    "    RandomGamma,\n",
    "    RGBShift,\n",
    ")\n",
    "\n",
    "# Augmentation (no normalization/ToTensor here - handled in dataset)\n",
    "augmentation = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    RandomGamma(gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
    "    RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:05.144883Z",
     "iopub.status.busy": "2025-12-16T08:12:05.144487Z",
     "iopub.status.idle": "2025-12-16T08:12:05.154781Z",
     "shell.execute_reply": "2025-12-16T08:12:05.153955Z",
     "shell.execute_reply.started": "2025-12-16T08:12:05.144858Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir, resize=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.resize = resize\n",
    "        self.transform = transform\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "        \n",
    "        # Normalization for pretrained models\n",
    "        self.normalize = A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def read_mask(self, mask_path):\n",
    "        image = cv2.imread(mask_path)\n",
    "        image = cv2.resize(image, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Red mask for neoplastic (class 1)\n",
    "        lower_red1 = np.array([0, 100, 20])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([160, 100, 20])\n",
    "        upper_red2 = np.array([179, 255, 255])\n",
    "        \n",
    "        lower_mask_red = cv2.inRange(image, lower_red1, upper_red1)\n",
    "        upper_mask_red = cv2.inRange(image, lower_red2, upper_red2)\n",
    "        \n",
    "        red_mask = lower_mask_red + upper_mask_red\n",
    "        red_mask[red_mask != 0] = 1\n",
    "\n",
    "        # Green mask for non-neoplastic (class 2)\n",
    "        green_mask = cv2.inRange(image, (36, 25, 25), (70, 255, 255))\n",
    "        green_mask[green_mask != 0] = 2\n",
    "\n",
    "        full_mask = cv2.bitwise_or(red_mask, green_mask)\n",
    "        full_mask = full_mask.astype(np.uint8)\n",
    "        \n",
    "        return full_mask  # Shape: (H, W), values: 0, 1, 2\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.images[idx])\n",
    "        \n",
    "        # Load and resize image\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        image = cv2.resize(image, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        # Load mask\n",
    "        label = self.read_mask(label_path)\n",
    "        \n",
    "        # Apply augmentation if provided\n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image, mask=label)\n",
    "            image = transformed[\"image\"]\n",
    "            label = transformed[\"mask\"]\n",
    "        \n",
    "        # Normalize and convert to tensor\n",
    "        normalized = self.normalize(image=image, mask=label)\n",
    "        image = normalized[\"image\"]  # torch.FloatTensor (C, H, W)\n",
    "        label = normalized[\"mask\"].long()  # torch.LongTensor (H, W)\n",
    "            \n",
    "        return image, label\n",
    "    \n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, img_dir=\"path/to/data\", resize=None, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.resize = resize\n",
    "        self.images = os.listdir(self.img_dir)\n",
    "        \n",
    "        # Normalization for pretrained models\n",
    "        self.normalize = A.Compose([\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.images[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        height, width, _ = image.shape\n",
    "        image = cv2.resize(image, self.resize, interpolation=cv2.INTER_AREA)\n",
    "        name = os.path.basename(img_path).split('.')[0]\n",
    "        \n",
    "        if self.transform:\n",
    "            transformed = self.transform(image=image)\n",
    "            image = transformed[\"image\"]\n",
    "        \n",
    "        # Normalize and convert to tensor\n",
    "        normalized = self.normalize(image=image)\n",
    "        image = normalized[\"image\"]  # torch.FloatTensor (C, H, W)\n",
    "        \n",
    "        return image, height, width, name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:07.926174Z",
     "iopub.status.busy": "2025-12-16T08:12:07.925854Z",
     "iopub.status.idle": "2025-12-16T08:12:07.930310Z",
     "shell.execute_reply": "2025-12-16T08:12:07.929325Z",
     "shell.execute_reply.started": "2025-12-16T08:12:07.926151Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "images_path = \"/kaggle/input/bkai-igh-neopolyp/train/train/\"\n",
    "masks_path =  \"/kaggle/input/bkai-igh-neopolyp/train_gt/train_gt/\"\n",
    "test_images_path = \"/kaggle/input/bkai-igh-neopolyp/test/test/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:15.745989Z",
     "iopub.status.busy": "2025-12-16T08:12:15.745050Z",
     "iopub.status.idle": "2025-12-16T08:12:17.803294Z",
     "shell.execute_reply": "2025-12-16T08:12:17.802564Z",
     "shell.execute_reply.started": "2025-12-16T08:12:15.745956Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "dataset = TrainDataset(images_path, masks_path, resize=(512, 512), transform=augmentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize dataset samples and check label values\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def denormalize(img_tensor):\n",
    "    \"\"\"Denormalize image for visualization\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return (img_tensor * std + mean).clamp(0, 1)\n",
    "\n",
    "# Check a few samples from the dataset\n",
    "print(\"=\" * 60)\n",
    "print(\"Dataset Diagnostics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "\n",
    "# Check first 5 samples\n",
    "fig, axes = plt.subplots(5, 3, figsize=(12, 20))\n",
    "axes[0, 0].set_title('Image', fontsize=12)\n",
    "axes[0, 1].set_title('Mask (raw values)', fontsize=12)\n",
    "axes[0, 2].set_title('Mask (colored)', fontsize=12)\n",
    "\n",
    "# Color map for visualization\n",
    "colors = np.array([\n",
    "    [0, 0, 0],       # 0: Background - black\n",
    "    [255, 0, 0],     # 1: Neoplastic - red  \n",
    "    [0, 255, 0],     # 2: Non-neoplastic - green\n",
    "], dtype=np.uint8)\n",
    "\n",
    "def mask_to_rgb_viz(mask):\n",
    "    \"\"\"Convert class mask to RGB, handle out-of-range values\"\"\"\n",
    "    h, w = mask.shape\n",
    "    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for c in range(min(len(colors), mask.max() + 1)):\n",
    "        rgb[mask == c] = colors[c] if c < len(colors) else [128, 128, 128]\n",
    "    # Mark out-of-range values in yellow\n",
    "    rgb[mask >= len(colors)] = [255, 255, 0]\n",
    "    return rgb\n",
    "\n",
    "for i in range(min(5, len(dataset))):\n",
    "    img, mask = dataset[i]\n",
    "    \n",
    "    # Now img is a tensor and mask is a tensor\n",
    "    unique_vals = torch.unique(mask).numpy()\n",
    "    print(f\"\\nSample {i}:\")\n",
    "    print(f\"  Image shape: {img.shape}, dtype: {img.dtype}\")\n",
    "    print(f\"  Mask shape: {mask.shape}, dtype: {mask.dtype}\")\n",
    "    print(f\"  Mask unique values: {unique_vals}\")\n",
    "    print(f\"  Mask min: {mask.min().item()}, max: {mask.max().item()}\")\n",
    "    \n",
    "    if mask.max() >= NUM_CLASSES:\n",
    "        print(f\"  ⚠️ WARNING: Mask contains values >= NUM_CLASSES ({NUM_CLASSES})\")\n",
    "    else:\n",
    "        print(f\"  ✓ Mask values are valid (0 to {NUM_CLASSES-1})\")\n",
    "    \n",
    "    # Display\n",
    "    img_display = denormalize(img).permute(1, 2, 0).numpy()\n",
    "    mask_np = mask.numpy()\n",
    "    \n",
    "    axes[i, 0].imshow(img_display)\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Show raw mask values\n",
    "    im = axes[i, 1].imshow(mask_np, cmap='tab20', vmin=0, vmax=max(3, mask_np.max()))\n",
    "    axes[i, 1].axis('off')\n",
    "    plt.colorbar(im, ax=axes[i, 1], fraction=0.046)\n",
    "    \n",
    "    # Show colored mask\n",
    "    axes[i, 2].imshow(mask_to_rgb_viz(mask_np))\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Dataset is ready for training!\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:17.804802Z",
     "iopub.status.busy": "2025-12-16T08:12:17.804576Z",
     "iopub.status.idle": "2025-12-16T08:12:17.811758Z",
     "shell.execute_reply": "2025-12-16T08:12:17.810897Z",
     "shell.execute_reply.started": "2025-12-16T08:12:17.804783Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7d653d91e7b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:19.319018Z",
     "iopub.status.busy": "2025-12-16T08:12:19.318209Z",
     "iopub.status.idle": "2025-12-16T08:12:19.337427Z",
     "shell.execute_reply": "2025-12-16T08:12:19.336537Z",
     "shell.execute_reply.started": "2025-12-16T08:12:19.318988Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = torch.utils.data.random_split(dataset, [train_size, val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:21.854918Z",
     "iopub.status.busy": "2025-12-16T08:12:21.854571Z",
     "iopub.status.idle": "2025-12-16T08:12:21.859548Z",
     "shell.execute_reply": "2025-12-16T08:12:21.858674Z",
     "shell.execute_reply.started": "2025-12-16T08:12:21.854889Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:24.336617Z",
     "iopub.status.busy": "2025-12-16T08:12:24.335785Z",
     "iopub.status.idle": "2025-12-16T08:12:24.340412Z",
     "shell.execute_reply": "2025-12-16T08:12:24.339567Z",
     "shell.execute_reply.started": "2025-12-16T08:12:24.336586Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from albumentations import (\n",
    "    Compose,\n",
    "    RandomRotate90,\n",
    "    Flip,\n",
    "    Transpose,\n",
    "    ElasticTransform,\n",
    "    GridDistortion,\n",
    "    OpticalDistortion,\n",
    "    RandomBrightnessContrast,\n",
    "    HorizontalFlip,\n",
    "    VerticalFlip,\n",
    "    RandomGamma,\n",
    "    RGBShift,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:25.832349Z",
     "iopub.status.busy": "2025-12-16T08:12:25.831704Z",
     "iopub.status.idle": "2025-12-16T08:12:25.836906Z",
     "shell.execute_reply": "2025-12-16T08:12:25.835913Z",
     "shell.execute_reply.started": "2025-12-16T08:12:25.832319Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "augmentation = Compose([\n",
    "    HorizontalFlip(p=0.5),\n",
    "    VerticalFlip(p=0.5),\n",
    "    RandomGamma (gamma_limit=(70, 130), eps=None, always_apply=False, p=0.2),\n",
    "    RGBShift(p=0.3, r_shift_limit=10, g_shift_limit=10, b_shift_limit=10),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:12:27.264877Z",
     "iopub.status.busy": "2025-12-16T08:12:27.264189Z",
     "iopub.status.idle": "2025-12-16T08:12:28.091899Z",
     "shell.execute_reply": "2025-12-16T08:12:28.090952Z",
     "shell.execute_reply.started": "2025-12-16T08:12:27.264848Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f32c9032e8c4a39bb2482ae8abaff65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d671265fc7549528acdc25e13ee38bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/15.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of SegformerForSemanticSegmentation were not initialized from the model checkpoint at nvidia/segformer-b0-finetuned-ade-512-512 and are newly initialized because the shapes did not match:\n",
      "- decode_head.classifier.bias: found shape torch.Size([150]) in the checkpoint and torch.Size([3]) in the model instantiated\n",
      "- decode_head.classifier.weight: found shape torch.Size([150, 256, 1, 1]) in the checkpoint and torch.Size([3, 256, 1, 1]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = SegformerForSemanticSegmentation.from_pretrained(\n",
    "    \"nvidia/mit-b3\",\n",
    "    num_labels=NUM_CLASSES,\n",
    "    ignore_mismatched_sizes=True\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CE + Dice Loss for semantic segmentation\n",
    "import torch.nn as nn\n",
    "\n",
    "def one_hot(labels, num_classes, device, dtype):\n",
    "    \"\"\"Convert labels (B, H, W) to one-hot (B, C, H, W)\"\"\"\n",
    "    shape = labels.shape\n",
    "    one_hot = torch.zeros(shape[0], num_classes, *shape[1:], device=device, dtype=dtype)\n",
    "    one_hot.scatter_(1, labels.unsqueeze(1), 1)\n",
    "    return one_hot\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    \"\"\"Dice Loss for multi-class segmentation\"\"\"\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        # input: (B, C, H, W) logits\n",
    "        # target: (B, H, W) class indices\n",
    "        num_classes = input.shape[1]\n",
    "        input_soft = F.softmax(input, dim=1)\n",
    "        target_one_hot = one_hot(target, num_classes, input.device, input.dtype)\n",
    "        \n",
    "        dims = (2, 3)\n",
    "        intersection = torch.sum(input_soft * target_one_hot, dims)\n",
    "        cardinality = torch.sum(input_soft + target_one_hot, dims)\n",
    "        \n",
    "        dice_score = 2.0 * intersection / (cardinality + self.eps)\n",
    "        return 1.0 - dice_score.mean()\n",
    "\n",
    "class CEDiceLoss(nn.Module):\n",
    "    \"\"\"Combined Cross-Entropy and Dice Loss\"\"\"\n",
    "    def __init__(self, ce_weight=0.5, dice_weight=0.5, class_weights=None):\n",
    "        super().__init__()\n",
    "        self.ce_weight = ce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.ce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "        self.dice_loss = DiceLoss()\n",
    "    \n",
    "    def forward(self, input, target):\n",
    "        ce = self.ce_loss(input, target)\n",
    "        dice = self.dice_loss(input, target)\n",
    "        return self.ce_weight * ce + self.dice_weight * dice\n",
    "\n",
    "# Initialize loss and optimizer\n",
    "class_weights = torch.tensor([0.2, 0.4, 0.4]).to(DEVICE)  # background, neoplastic, non-neoplastic\n",
    "loss_function = CEDiceLoss(ce_weight=0.5, dice_weight=0.5, class_weights=class_weights)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-12-16T08:15:05.797178Z",
     "iopub.status.busy": "2025-12-16T08:15:05.796296Z",
     "iopub.status.idle": "2025-12-16T08:15:42.778267Z",
     "shell.execute_reply": "2025-12-16T08:15:42.777152Z",
     "shell.execute_reply.started": "2025-12-16T08:15:05.797138Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/57 [00:36<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[0;32m----> 5\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m, y\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      6\u001b[0m     out \u001b[38;5;241m=\u001b[39m model(pixel_values\u001b[38;5;241m=\u001b[39mx, labels\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mloss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "# Training loop with validation and Dice evaluation\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_dice_per_class(preds, targets, num_classes, eps=1e-6):\n",
    "    \"\"\"Calculate Dice score per class\"\"\"\n",
    "    dice_scores = []\n",
    "    for c in range(num_classes):\n",
    "        pred_c = (preds == c).float()\n",
    "        target_c = (targets == c).float()\n",
    "        intersection = (pred_c * target_c).sum()\n",
    "        union = pred_c.sum() + target_c.sum()\n",
    "        dice = (2.0 * intersection + eps) / (union + eps)\n",
    "        dice_scores.append(dice.item())\n",
    "    return dice_scores\n",
    "\n",
    "best_val_dice = 0.0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_dices = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    t0 = time.time()\n",
    "    \n",
    "    # ===== Training =====\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    \n",
    "    for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Train\"):\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(pixel_values=x)\n",
    "        logits = outputs.logits  # (B, C, h, w)\n",
    "        \n",
    "        # Upsample logits to target size\n",
    "        logits = F.interpolate(logits, size=y.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        \n",
    "        loss = loss_function(logits, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader)\n",
    "    train_losses.append(train_loss)\n",
    "    \n",
    "    # ===== Validation =====\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_dice_scores = [[] for _ in range(NUM_CLASSES)]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x_val, y_val in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{EPOCHS} - Val\"):\n",
    "            x_val = x_val.to(DEVICE)\n",
    "            y_val = y_val.to(DEVICE)\n",
    "            \n",
    "            logits_val = model(pixel_values=x_val).logits\n",
    "            logits_val = F.interpolate(logits_val, size=y_val.shape[-2:], mode='bilinear', align_corners=False)\n",
    "            \n",
    "            loss_v = loss_function(logits_val, y_val)\n",
    "            val_loss += loss_v.item()\n",
    "            \n",
    "            preds = torch.argmax(logits_val, dim=1)\n",
    "            dice_scores = calculate_dice_per_class(preds, y_val, NUM_CLASSES)\n",
    "            for c in range(NUM_CLASSES):\n",
    "                all_dice_scores[c].append(dice_scores[c])\n",
    "    \n",
    "    val_loss /= len(val_loader)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Calculate mean Dice per class\n",
    "    mean_dice_per_class = [np.mean(scores) for scores in all_dice_scores]\n",
    "    mean_dice = np.mean(mean_dice_per_class)\n",
    "    val_dices.append(mean_dice)\n",
    "    \n",
    "    scheduler.step()\n",
    "    elapsed = time.time() - t0\n",
    "    \n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS} ({elapsed:.1f}s)\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  Dice - Background: {mean_dice_per_class[0]:.4f}, Neoplastic: {mean_dice_per_class[1]:.4f}, Non-neoplastic: {mean_dice_per_class[2]:.4f}\")\n",
    "    print(f\"  Mean Dice: {mean_dice:.4f}\")\n",
    "    \n",
    "    # Save best model\n",
    "    if mean_dice > best_val_dice:\n",
    "        best_val_dice = mean_dice\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_dice': mean_dice,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"  ✓ Saved best model (Dice: {mean_dice:.4f})\")\n",
    "\n",
    "print(f\"\\n Training complete! Best Val Dice: {best_val_dice:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, label='Train Loss', color='blue')\n",
    "axes[0].plot(val_losses, label='Val Loss', color='orange')\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training & Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Dice curve\n",
    "axes[1].plot(val_dices, label='Val Dice', color='green')\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Dice Score')\n",
    "axes[1].set_title('Validation Dice Score')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True)\n",
    "\n",
    "# Final metrics\n",
    "axes[2].axis('off')\n",
    "axes[2].text(0.1, 0.8, f\"Final Results:\", fontsize=14, fontweight='bold')\n",
    "axes[2].text(0.1, 0.6, f\"Best Val Dice: {best_val_dice:.4f}\", fontsize=12)\n",
    "axes[2].text(0.1, 0.4, f\"Final Train Loss: {train_losses[-1]:.4f}\", fontsize=12)\n",
    "axes[2].text(0.1, 0.2, f\"Final Val Loss: {val_losses[-1]:.4f}\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/training_curves.png', dpi=150)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize sample predictions on validation set\n",
    "def denormalize(img_tensor):\n",
    "    \"\"\"Denormalize image for visualization\"\"\"\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    return (img_tensor.cpu() * std + mean).clamp(0, 1)\n",
    "\n",
    "# Color map for 3 classes: background (black), neoplastic (red), non-neoplastic (green)\n",
    "colors = np.array([\n",
    "    [0, 0, 0],       # Background - black\n",
    "    [255, 0, 0],     # Neoplastic - red  \n",
    "    [0, 255, 0],     # Non-neoplastic - green\n",
    "], dtype=np.uint8)\n",
    "\n",
    "def mask_to_rgb(mask):\n",
    "    \"\"\"Convert class mask to RGB image\"\"\"\n",
    "    h, w = mask.shape\n",
    "    rgb = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "    for c in range(len(colors)):\n",
    "        rgb[mask == c] = colors[c]\n",
    "    return rgb\n",
    "\n",
    "# Get some validation samples\n",
    "model.eval()\n",
    "fig, axes = plt.subplots(4, 3, figsize=(12, 16))\n",
    "axes[0, 0].set_title('Input Image', fontsize=12)\n",
    "axes[0, 1].set_title('Ground Truth', fontsize=12)\n",
    "axes[0, 2].set_title('Prediction', fontsize=12)\n",
    "\n",
    "val_iter = iter(val_loader)\n",
    "for i in range(4):\n",
    "    try:\n",
    "        x_val, y_val = next(val_iter)\n",
    "    except StopIteration:\n",
    "        break\n",
    "    \n",
    "    x_val = x_val.to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        logits = model(pixel_values=x_val).logits\n",
    "        logits = F.interpolate(logits, size=y_val.shape[-2:], mode='bilinear', align_corners=False)\n",
    "        pred = torch.argmax(logits, dim=1).cpu().numpy()[0]\n",
    "    \n",
    "    # Display\n",
    "    img_display = denormalize(x_val[0]).permute(1, 2, 0).numpy()\n",
    "    gt_display = mask_to_rgb(y_val[0].numpy())\n",
    "    pred_display = mask_to_rgb(pred)\n",
    "    \n",
    "    axes[i, 0].imshow(img_display)\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 1].imshow(gt_display)\n",
    "    axes[i, 1].axis('off')\n",
    "    axes[i, 2].imshow(pred_display)\n",
    "    axes[i, 2].axis('off')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [\n",
    "    Patch(facecolor='black', label='Background'),\n",
    "    Patch(facecolor='red', label='Neoplastic'),\n",
    "    Patch(facecolor='green', label='Non-neoplastic')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/validation_samples.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T06:59:18.059289Z",
     "iopub.status.busy": "2023-11-17T06:59:18.059027Z",
     "iopub.status.idle": "2023-11-17T06:59:18.074789Z",
     "shell.execute_reply": "2023-11-17T06:59:18.074081Z",
     "shell.execute_reply.started": "2023-11-17T06:59:18.059247Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Test Dataset for inference\n",
    "from torchvision.transforms import Resize, ToPILImage, InterpolationMode\n",
    "import cv2\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, images_path):\n",
    "        super().__init__()\n",
    "        self.images_list = sorted([os.path.join(images_path, f) for f in os.listdir(images_path)])\n",
    "        self.transform = A.Compose([\n",
    "            A.Resize(512, 512),\n",
    "            A.Normalize(),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.images_list[index]\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        original_size = (img.size[1], img.size[0])  # (H, W)\n",
    "        transformed = self.transform(image=np.array(img))\n",
    "        return transformed[\"image\"], img_path, original_size[0], original_size[1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images_list)\n",
    "\n",
    "# Create test dataloader\n",
    "test_dataset = TestDataset(test_images_path)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T07:16:24.408947Z",
     "iopub.status.busy": "2023-11-17T07:16:24.408653Z",
     "iopub.status.idle": "2023-11-17T07:16:40.900046Z",
     "shell.execute_reply": "2023-11-17T07:16:40.898779Z",
     "shell.execute_reply.started": "2023-11-17T07:16:24.408921Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Run inference on test set and save predictions\n",
    "import os\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(f\"Loaded model with Val Dice: {checkpoint['val_dice']:.4f}\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"/kaggle/working/predicted_masks\"\n",
    "if not os.path.isdir(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch_idx, (imgs, paths, heights, widths) in enumerate(tqdm(test_dataloader, desc=\"Inference\")):\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        \n",
    "        # Get predictions\n",
    "        logits = model(pixel_values=imgs).logits\n",
    "        \n",
    "        # Process each image in batch\n",
    "        for i in range(len(paths)):\n",
    "            h, w = heights[i].item(), widths[i].item()\n",
    "            \n",
    "            # Upsample to original size\n",
    "            logit = logits[i:i+1]\n",
    "            logit = F.interpolate(logit, size=(h, w), mode='bilinear', align_corners=False)\n",
    "            pred = torch.argmax(logit, dim=1).squeeze().cpu().numpy().astype(np.uint8)\n",
    "            \n",
    "            # Convert to color mask for visualization\n",
    "            pred_rgb = mask_to_rgb(pred)\n",
    "            \n",
    "            # Save prediction\n",
    "            image_id = os.path.basename(paths[i]).split('.')[0]\n",
    "            filename = f\"{image_id}.png\"\n",
    "            \n",
    "            # Save as PNG\n",
    "            pred_img = Image.fromarray(pred_rgb)\n",
    "            pred_img.save(os.path.join(output_dir, filename))\n",
    "\n",
    "print(f\"Saved {len(test_dataset)} predictions to {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-17T07:16:40.902041Z",
     "iopub.status.busy": "2023-11-17T07:16:40.901648Z",
     "iopub.status.idle": "2023-11-17T07:16:43.898877Z",
     "shell.execute_reply": "2023-11-17T07:16:43.897848Z",
     "shell.execute_reply.started": "2023-11-17T07:16:40.901995Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create submission CSV with RLE encoding\n",
    "def rle_to_string(runs):\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_encode_one_mask(mask):\n",
    "    pixels = mask.flatten()\n",
    "    pixels[pixels > 0] = 255\n",
    "    use_padding = False\n",
    "    if pixels[0] or pixels[-1]:\n",
    "        use_padding = True\n",
    "        pixel_padded = np.zeros([len(pixels) + 2], dtype=pixels.dtype)\n",
    "        pixel_padded[1:-1] = pixels\n",
    "        pixels = pixel_padded\n",
    "    \n",
    "    rle = np.where(pixels[1:] != pixels[:-1])[0] + 2\n",
    "    if use_padding:\n",
    "        rle = rle - 1\n",
    "    rle[1::2] = rle[1::2] - rle[:-1:2]\n",
    "    return rle_to_string(rle)\n",
    "\n",
    "def mask2string(mask_dir):\n",
    "    \"\"\"Convert masks to RLE strings for submission\"\"\"\n",
    "    strings = []\n",
    "    ids = []\n",
    "    \n",
    "    for image_id in sorted(os.listdir(mask_dir)):\n",
    "        id_name = image_id.split('.')[0]\n",
    "        path = os.path.join(mask_dir, image_id)\n",
    "        img = cv2.imread(path)[:,:,::-1]  # BGR to RGB\n",
    "        \n",
    "        # Channel 0: neoplastic (red channel)\n",
    "        # Channel 1: non-neoplastic (green channel)\n",
    "        for channel in range(2):\n",
    "            ids.append(f'{id_name}_{channel}')\n",
    "            string = rle_encode_one_mask(img[:,:,channel])\n",
    "            strings.append(string)\n",
    "    \n",
    "    return {'ids': ids, 'strings': strings}\n",
    "\n",
    "# Generate submission\n",
    "MASK_DIR_PATH = '/kaggle/working/predicted_masks'\n",
    "res = mask2string(MASK_DIR_PATH)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Id': res['ids'],\n",
    "    'Expected': res['strings']\n",
    "})\n",
    "df.to_csv('/kaggle/working/submission.csv', index=False)\n",
    "print(f\"Submission saved with {len(df)} entries\")\n",
    "df.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize some test predictions\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 15))\n",
    "axes[0, 0].set_title('Test Image', fontsize=12)\n",
    "axes[0, 1].set_title('Prediction', fontsize=12)\n",
    "\n",
    "test_iter = iter(test_dataloader)\n",
    "imgs, paths, heights, widths = next(test_iter)\n",
    "imgs = imgs.to(DEVICE)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    logits = model(pixel_values=imgs).logits\n",
    "\n",
    "for i in range(min(3, len(imgs))):\n",
    "    # Original image\n",
    "    original_img = Image.open(paths[i]).convert(\"RGB\")\n",
    "    \n",
    "    # Prediction\n",
    "    h, w = heights[i].item(), widths[i].item()\n",
    "    logit = logits[i:i+1]\n",
    "    logit = F.interpolate(logit, size=(h, w), mode='bilinear', align_corners=False)\n",
    "    pred = torch.argmax(logit, dim=1).squeeze().cpu().numpy().astype(np.uint8)\n",
    "    pred_rgb = mask_to_rgb(pred)\n",
    "    \n",
    "    axes[i, 0].imshow(original_img)\n",
    "    axes[i, 0].axis('off')\n",
    "    axes[i, 1].imshow(pred_rgb)\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "# Add legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor='black', label='Background'),\n",
    "    Patch(facecolor='red', label='Neoplastic'),\n",
    "    Patch(facecolor='green', label='Non-neoplastic')\n",
    "]\n",
    "fig.legend(handles=legend_elements, loc='lower center', ncol=3, fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('/kaggle/working/test_predictions.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 2715462,
     "sourceId": 30892,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30588,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
